{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed3f82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[2061,  318,  262, 3139,  286, 4881,   30,  198,  464, 3139,  286, 4881,\n",
      "          318, 6342,   13]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1')}\n",
      "input_len: 15\n",
      "prompt_len: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/xwj_llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor logits shape: torch.Size([1, 15, 50257])\n",
      "reference logits shape: torch.Size([1, 15, 50257])\n",
      "actor logits shape: torch.Size([1, 14, 50257])\n",
      "reference logits shape: torch.Size([1, 14, 50257])\n",
      "log_probs_actor: torch.Size([1, 14, 50257])\n",
      "log_probs_reference: torch.Size([1, 14, 50257])\n",
      "logp_actor shape: torch.Size([1, 14])\n",
      "logp_reference shape: torch.Size([1, 14])\n",
      "KL Divergence: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "\n",
    "\n",
    "def compute_response_kl(\n",
    "    actor_model: PreTrainedModel, \n",
    "    reference_model: PreTrainedModel, \n",
    "    tokenizer: PreTrainedTokenizer, \n",
    "    prompt: str, \n",
    "    response: str, \n",
    "    device: str = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    ") -> float:\n",
    "    full_text = prompt + response\n",
    "    inputs = tokenizer(full_text, return_tensors='pt').to(device)\n",
    "    print(f\"inputs: {inputs}\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    print(f\"input_len: {input_ids.shape[-1]}\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        prompt_ids = tokenizer(prompt, return_tensors='pt')[\"input_ids\"].to(device)\n",
    "    prompt_len = prompt_ids.shape[-1]\n",
    "    print(f\"prompt_len: {prompt_len}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        actor_logits = actor_model(input_ids).logits\n",
    "        reference_logits = reference_model(input_ids).logits\n",
    "        print(f\"actor logits shape: {actor_logits.shape}\")\n",
    "        print(f\"reference logits shape: {reference_logits.shape}\")\n",
    "\n",
    "    \"\"\"下一个词是当前词预测的label，满足自回归性质\n",
    "        去掉最后一个token，因为最后一个token没有下一个预测，也就是没有label\"\"\"\n",
    "    actor_logits = actor_logits[:, :-1, :]\n",
    "    reference_logits = reference_logits[:, :-1, :]\n",
    "    print(f\"actor logits shape: {actor_logits.shape}\")\n",
    "    print(f\"reference logits shape: {reference_logits.shape}\")\n",
    "    labels = input_ids[:, 1: ] # 去掉第一个token，因为第一个token不是label\n",
    "\n",
    "    log_probs_actor = F.log_softmax(actor_logits, dim=-1)\n",
    "    log_probs_reference = F.log_softmax(reference_logits, dim=-1)\n",
    "    # 取log\n",
    "    print(f\"log_probs_actor: {log_probs_actor.shape}\")\n",
    "    print(f\"log_probs_reference: {log_probs_reference.shape}\")\n",
    "\n",
    "    logp_actor = log_probs_actor.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
    "    logp_reference = log_probs_reference.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
    "    print(f\"logp_actor shape: {logp_actor.shape}\")\n",
    "    print(f\"logp_reference shape: {logp_reference.shape}\")\n",
    "       \n",
    "    response_mask = torch.zeros_like(labels, dtype=torch.float)\n",
    "    response_mask[:, prompt_len - 1:] = 1.0  # prompt_len-1 开始是 response 的第一个 token\n",
    "\n",
    "    # 只关注生成阶段的token\n",
    "    kl_tokenwise = (logp_actor - logp_reference) * response_mask\n",
    "    kl_total = kl_tokenwise.sum().item()  # 总 KL\n",
    "\n",
    "    # 可选：返回平均 KL\n",
    "    # avg_kl = kl_total / response_mask.sum().item()\n",
    "\n",
    "    return kl_total\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    actor_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").eval().to(\"cuda:1\")\n",
    "    ref_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").eval().to(\"cuda:1\")\n",
    "\n",
    "    prompt = \"What is the capital of France?\\n\"\n",
    "    response = \"The capital of France is Paris.\"\n",
    "\n",
    "    kl = compute_response_kl(actor_model, ref_model, tokenizer, prompt, response)\n",
    "    print(f\"KL Divergence: {kl:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xwj_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
